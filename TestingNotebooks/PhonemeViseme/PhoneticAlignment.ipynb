{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa0aebb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!mfa model download acoustic english_us_arpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02c1249f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!mfa model download dictionary english_us_arpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "903f1765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\u001b[1m Acoustic model:\u001b[0m \u001b[32menglish_us_arpa_None\u001b[0m\r\n",
      "  -\u001b[1m Version:\u001b[0m \u001b[31m2.0.0rc4.dev19+ged818cb.d20220404\u001b[0m\r\n",
      "  -\u001b[1m Train date:\u001b[0m \u001b[36m2022-05-11 11:19:39.314871\u001b[0m\r\n",
      "  -\u001b[1m Architecture:\u001b[0m \u001b[36mgmm-hmm\u001b[0m\r\n",
      "  -\u001b[1m Phone type:\u001b[0m \u001b[36mtriphone\u001b[0m\r\n",
      "  -\u001b[1m Features:\u001b[0m \u001b[36m\u001b[0m\r\n",
      "    -\u001b[1m Feature type:\u001b[0m \u001b[36mmfcc\u001b[0m\r\n",
      "    -\u001b[1m Frame shift:\u001b[0m \u001b[36m10\u001b[0m\r\n",
      "    -\u001b[1m Performs speaker adaptation:\u001b[0m \u001b[32mTrue\u001b[0m\r\n",
      "    -\u001b[1m Performs LDA on features:\u001b[0m \u001b[32mTrue\u001b[0m\r\n",
      "\r\n",
      "  -\u001b[1m Phones:\u001b[0m \u001b[36mAA\u001b[0m, \u001b[36mAA0\u001b[0m, \u001b[36mAA1\u001b[0m, \u001b[36mAA2\u001b[0m, \u001b[36mAE\u001b[0m, \u001b[36mAE0\u001b[0m, \u001b[36mAE1\u001b[0m, \u001b[36mAE2\u001b[0m, \u001b[36mAH\u001b[0m, \u001b[36mAH0\u001b[0m, \u001b[36mAH1\u001b[0m, \u001b[36mAH2\u001b[0m, \u001b[36mAO\u001b[0m, \u001b[36mAO0\u001b[0m, \u001b[36mAO1\u001b[0m, \u001b[36mAO2\u001b[0m, \u001b[36mAW\u001b[0m, \u001b[36mAW0\u001b[0m, \u001b[36mAW1\u001b[0m, \u001b[36mAW2\u001b[0m, \u001b[36mAY\u001b[0m, \u001b[36mAY0\u001b[0m, \u001b[36mAY1\u001b[0m,\r\n",
      "            \u001b[36mAY2\u001b[0m, \u001b[36mB\u001b[0m, \u001b[36mCH\u001b[0m, \u001b[36mD\u001b[0m, \u001b[36mDH\u001b[0m, \u001b[36mEH\u001b[0m, \u001b[36mEH0\u001b[0m, \u001b[36mEH1\u001b[0m, \u001b[36mEH2\u001b[0m, \u001b[36mER\u001b[0m, \u001b[36mER0\u001b[0m, \u001b[36mER1\u001b[0m, \u001b[36mER2\u001b[0m, \u001b[36mEY\u001b[0m, \u001b[36mEY0\u001b[0m, \u001b[36mEY1\u001b[0m, \u001b[36mEY2\u001b[0m, \u001b[36mF\u001b[0m, \u001b[36mG\u001b[0m, \u001b[36mHH\u001b[0m, \u001b[36mIH\u001b[0m, \u001b[36mIH0\u001b[0m, \u001b[36mIH1\u001b[0m, \u001b[36mIH2\u001b[0m, \u001b[36mIY\u001b[0m,\r\n",
      "            \u001b[36mIY0\u001b[0m, \u001b[36mIY1\u001b[0m, \u001b[36mIY2\u001b[0m, \u001b[36mJH\u001b[0m, \u001b[36mK\u001b[0m, \u001b[36mL\u001b[0m, \u001b[36mM\u001b[0m, \u001b[36mN\u001b[0m, \u001b[36mNG\u001b[0m, \u001b[36mOW\u001b[0m, \u001b[36mOW0\u001b[0m, \u001b[36mOW1\u001b[0m, \u001b[36mOW2\u001b[0m, \u001b[36mOY\u001b[0m, \u001b[36mOY0\u001b[0m, \u001b[36mOY1\u001b[0m, \u001b[36mOY2\u001b[0m, \u001b[36mP\u001b[0m, \u001b[36mR\u001b[0m, \u001b[36mS\u001b[0m, \u001b[36mSH\u001b[0m, \u001b[36mT\u001b[0m, \u001b[36mTH\u001b[0m, \u001b[36mUH\u001b[0m, \u001b[36mUH0\u001b[0m, \u001b[36mUH1\u001b[0m,\r\n",
      "            \u001b[36mUH2\u001b[0m, \u001b[36mUW\u001b[0m, \u001b[36mUW0\u001b[0m, \u001b[36mUW1\u001b[0m, \u001b[36mUW2\u001b[0m, \u001b[36mV\u001b[0m, \u001b[36mW\u001b[0m, \u001b[36mY\u001b[0m, \u001b[36mZ\u001b[0m, and \u001b[36mZH\u001b[0m\r\n",
      "\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!mfa model inspect acoustic english_us_arpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aee253f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_utterance_path = '/Users/romitbarua/Documents/Berkeley/Fall 2022/EE225 - Audio Signal Processing/ClassicalDeepFakeDetection/MultiModalDeepFake/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "948aab25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m - Setting up corpus information...\n",
      "\u001b[32mINFO\u001b[0m - Loading corpus from source files...\n",
      "100%|███████████████████████████████████████████| 67/67 [00:01<00:00, 64.12it/s]\n",
      "\u001b[32mINFO\u001b[0m - Found 67 speakers across 67 files, average number of utterances per speaker: 1.0\n",
      "\u001b[32mINFO\u001b[0m - Initializing multiprocessing jobs...\n",
      "\u001b[32mINFO\u001b[0m - Creating corpus split for feature generation...\n",
      "\u001b[32mINFO\u001b[0m - Generating base features (mfcc)...\n",
      "\u001b[32mINFO\u001b[0m - Generating MFCCs...\n",
      " 90%|██████████████████████████████████████▌    | 60/67 [00:04<00:00, 13.36it/s]\n",
      "\u001b[32mINFO\u001b[0m - Calculating CMVN...\n",
      "\u001b[32mINFO\u001b[0m - Creating corpus split with features...\n",
      "\u001b[32mINFO\u001b[0m - Skipping transcription testing\n",
      "\u001b[32mINFO\u001b[0m - Finished initializing!\n",
      "\n",
      "\u001b[32mINFO\u001b[0m - \u001b[1m******\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m - \u001b[1mCorpus\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m - \u001b[1m******\u001b[0m\n",
      "\n",
      "\u001b[32mINFO\u001b[0m -   \u001b[32m67\u001b[0m sound files\n",
      "\u001b[32mINFO\u001b[0m -   \u001b[32m0\u001b[0m text files\n",
      "\u001b[32mINFO\u001b[0m -   \u001b[32m67\u001b[0m speakers\n",
      "\u001b[32mINFO\u001b[0m -   \u001b[32m67\u001b[0m utterances\n",
      "\u001b[32mINFO\u001b[0m -   \u001b[32m449.720\u001b[0m seconds total duration\n",
      "\n",
      "\u001b[32mINFO\u001b[0m -   \u001b[1mSound file read errors\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m -   \u001b[1m======================\u001b[0m\n",
      "\n",
      "\u001b[32mINFO\u001b[0m -     There were \u001b[32mno\u001b[0m issues reading sound files.\n",
      "\n",
      "\u001b[32mINFO\u001b[0m -   \u001b[1mFeature generation\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m -   \u001b[1m==================\u001b[0m\n",
      "\n",
      "\u001b[32mINFO\u001b[0m -     There were \u001b[32mno\u001b[0m utterances missing features.\n",
      "\n",
      "\u001b[32mINFO\u001b[0m -   \u001b[1mFiles without transcriptions\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m -   \u001b[1m============================\u001b[0m\n",
      "\n",
      "\u001b[32mINFO\u001b[0m -     There were \u001b[32mno\u001b[0m sound files missing transcriptions.\n",
      "\n",
      "\u001b[32mINFO\u001b[0m -   \u001b[1mTranscriptions without sound files\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m -   \u001b[1m==================================\u001b[0m\n",
      "\n",
      "\u001b[32mINFO\u001b[0m -     There were \u001b[32mno\u001b[0m transcription files missing sound files.\n",
      "\n",
      "\n",
      "\u001b[32mINFO\u001b[0m - \u001b[1m**********\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m - \u001b[1mDictionary\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m - \u001b[1m**********\u001b[0m\n",
      "\n",
      "\u001b[32mINFO\u001b[0m -   \u001b[1mOut of vocabulary words\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m -   \u001b[1m=======================\u001b[0m\n",
      "\n",
      "\u001b[32mINFO\u001b[0m -     There were \u001b[33mno\u001b[0m missing words from the dictionary. If you plan on using the a model trained on this dataset to\n",
      "                align        other datasets in the future, it is recommended that there be at least some missing words.\n",
      "\n",
      "\u001b[32mINFO\u001b[0m -   \u001b[1mAcoustic model compatibility\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m -   \u001b[1m============================\u001b[0m\n",
      "\n",
      "\u001b[32mINFO\u001b[0m -     There were \u001b[32mno\u001b[0m phones in the dictionary without acoustic models.\n",
      "\n",
      "\u001b[32mINFO\u001b[0m - Compiling training graphs...\n",
      "100%|███████████████████████████████████████████| 67/67 [00:03<00:00, 17.76it/s]\n",
      "\u001b[32mINFO\u001b[0m - Generating alignments...\n",
      "100%|███████████████████████████████████████████| 67/67 [00:05<00:00, 11.38it/s]\n",
      "\u001b[32mINFO\u001b[0m - Calculating fMLLR for speaker adaptation...\n",
      "100%|███████████████████████████████████████████| 67/67 [00:03<00:00, 18.40it/s]\n",
      "\u001b[32mINFO\u001b[0m - Generating alignments...\n",
      "100%|███████████████████████████████████████████| 67/67 [00:05<00:00, 11.27it/s]\n",
      "\n",
      "\u001b[32mINFO\u001b[0m - \u001b[1m*********\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m - \u001b[1mAlignment\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m - \u001b[1m*********\u001b[0m\n",
      "\n",
      "\u001b[32mINFO\u001b[0m -   \u001b[32m0\u001b[0m utterances were too short to be aligned\n",
      "\u001b[32mINFO\u001b[0m -   \u001b[32m0\u001b[0m utterances that need a larger beam to align\n",
      "\u001b[32mINFO\u001b[0m -   \u001b[32m67\u001b[0m utterances were successfully aligned\n",
      "\u001b[32mINFO\u001b[0m - Done! Everything took 50.24152112007141 seconds\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!mfa validate \"/Users/romitbarua/Downloads/FakeAVCeleb_v1.2-20221128T063433Z-005/FakeAVCeleb_v1.2/RealVideo-RealAudio\" english_us_arpa english_us_arpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f1dcb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m - Setting up corpus information...\n",
      "\u001b[32mINFO\u001b[0m - Loading corpus from source files...\n",
      "100%|███████████████████████████████████████████| 67/67 [00:01<00:00, 63.95it/s]\n",
      "\u001b[32mINFO\u001b[0m - Found 67 speakers across 67 files, average number of utterances per speaker: 1.0\n",
      "\u001b[32mINFO\u001b[0m - Initializing multiprocessing jobs...\n",
      "\u001b[32mINFO\u001b[0m - Creating corpus split for feature generation...\n",
      "\u001b[32mINFO\u001b[0m - Generating base features (mfcc)...\n",
      "\u001b[32mINFO\u001b[0m - Generating MFCCs...\n",
      " 90%|██████████████████████████████████████▌    | 60/67 [00:04<00:00, 14.50it/s]\n",
      "\u001b[32mINFO\u001b[0m - Calculating CMVN...\n",
      "\u001b[32mINFO\u001b[0m - Creating corpus split with features...\n",
      "\u001b[32mINFO\u001b[0m - Compiling training graphs...\n",
      "100%|███████████████████████████████████████████| 67/67 [00:03<00:00, 18.48it/s]\n",
      "\u001b[32mINFO\u001b[0m - Performing first-pass alignment...\n",
      "\u001b[32mINFO\u001b[0m - Generating alignments...\n",
      "100%|███████████████████████████████████████████| 67/67 [00:05<00:00, 11.22it/s]\n",
      "\u001b[32mINFO\u001b[0m - Calculating fMLLR for speaker adaptation...\n",
      "100%|███████████████████████████████████████████| 67/67 [00:03<00:00, 18.05it/s]\n",
      "\u001b[32mINFO\u001b[0m - Performing second-pass alignment...\n",
      "\u001b[32mINFO\u001b[0m - Generating alignments...\n",
      "100%|███████████████████████████████████████████| 67/67 [00:05<00:00, 11.39it/s]\n",
      "\u001b[32mINFO\u001b[0m - Exporting TextGrids to /Users/romitbarua/Documents/Berkeley/Fall 2022/EE225 - Audio Signal\n",
      "                Processing/ClassicalDeepFakeDetection/MultiModalDeepFake/data/test_aligned_output...\n",
      "\u001b[32mINFO\u001b[0m - Collecting phone and word alignments from alignment lattices...\n",
      "100%|███████████████████████████████████████████| 67/67 [00:04<00:00, 16.65it/s]\n",
      "  0%|                                                    | 0/67 [00:03<?, ?it/s]\n",
      "\u001b[32mINFO\u001b[0m - Finished exporting TextGrids to /Users/romitbarua/Documents/Berkeley/Fall 2022/EE225 - Audio Signal\n",
      "                Processing/ClassicalDeepFakeDetection/MultiModalDeepFake/data/test_aligned_output!\n",
      "\u001b[32mINFO\u001b[0m - Done! Everything took 60.01854729652405 seconds\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!mfa align --clean \"/Users/romitbarua/Downloads/FakeAVCeleb_v1.2-20221128T063433Z-005/FakeAVCeleb_v1.2/RealVideo-RealAudio\" english_us_arpa english_us_arpa \"/Users/romitbarua/Documents/Berkeley/Fall 2022/EE225 - Audio Signal Processing/ClassicalDeepFakeDetection/MultiModalDeepFake/data/test_aligned_output\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
