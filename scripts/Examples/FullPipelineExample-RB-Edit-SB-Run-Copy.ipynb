{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d8bb43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.24.3) or chardet (5.1.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "2023-06-29 21:41:29.611293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-29 21:41:29.719320: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-29 21:41:30.274679: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-29 21:41:30.274731: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-29 21:41:30.274735: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "[NeMo W 2023-06-29 21:41:31 optimizers:55] Apex was not found. Using the lamb or fused_adam optimizer will error out.\n",
      "[NeMo W 2023-06-29 21:41:31 experimental:27] Module <class 'nemo.collections.asr.models.audio_to_audio_model.AudioToAudioModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-06-29 21:41:32 experimental:27] Module <class 'nemo.collections.asr.modules.audio_modules.SpectrogramToMultichannelFeatures'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-06-29 21:41:32 nemo_logging:349] /home/ubuntu/miniconda3/envs/nemo/lib/python3.8/site-packages/torch/jit/annotations.py:309: UserWarning: TorchScript will treat type annotations of Tensor dtype-specific subtypes as if they are normal Tensors. dtype constraints are not enforced in compilation either.\n",
      "      warnings.warn(\"TorchScript will treat type annotations of Tensor \"\n",
      "    \n",
      "[NeMo W 2023-06-29 21:41:32 experimental:27] Module <class 'nemo.collections.asr.data.audio_to_audio.BaseAudioDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-06-29 21:41:32 experimental:27] Module <class 'nemo.collections.asr.data.audio_to_audio.AudioToTargetDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-06-29 21:41:32 experimental:27] Module <class 'nemo.collections.asr.data.audio_to_audio.AudioToTargetWithReferenceDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-06-29 21:41:32 experimental:27] Module <class 'nemo.collections.asr.data.audio_to_audio.AudioToTargetWithEmbeddingDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-06-29 21:41:32 experimental:27] Module <class 'nemo.collections.asr.models.enhancement_models.EncMaskDecAudioToAudioModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/ubuntu/MultiModalDeepFake\")\n",
    "import nemo.collections.asr as nemo_asr \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7199e808",
   "metadata": {},
   "outputs": [],
   "source": [
    "from packages.TIMITDataLoader import TIMITDataLoader\n",
    "from packages.LJDataLoader import LJDataLoader\n",
    "from packages.AudioEmbeddingsManager import AudioEmbeddingsManager\n",
    "from packages.ModelManager import ModelManager\n",
    "from packages.CadenceModelManager import CadenceModelManager\n",
    "import packages.AnalysisManager as am\n",
    "from packages.SmileFeatureManager import SmileFeatureManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1a271a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b54c0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N real and fake phrases: 500, 500\n",
      "492 491\n",
      "/home/ubuntu/data/TIMIT_and_ElevenLabs/TIMIT and ElevenLabs/SI946/real/MPAR0_SI946.WAV\n",
      "# of Train instances: 589\n",
      "# of Dev instances: 196\n",
      "# of Test instances: 198\n"
     ]
    }
   ],
   "source": [
    "timit_data_loader = TIMITDataLoader('/home/ubuntu/data/TIMIT_and_ElevenLabs/TIMIT and ElevenLabs')\n",
    "#df = timit_data_loader.generate_split_speaker_test()\n",
    "df = timit_data_loader.generate_split()\n",
    "df['speaker'] = [item.split('/')[-1].split('_')[0] for item in df['path']]\n",
    "\n",
    "\n",
    "fake_voices = ['Adam', 'Antoni', 'Arnold', 'Bella', 'Biden', 'Domi', 'Elli', 'Josh', 'Obama', 'Rachel', 'Sam']\n",
    "\n",
    "real_speakers = list(set([item for item in df['speaker'] if not item.startswith(tuple(fake_voices))]))\n",
    "fake_speakers = list(set([item for item in df['speaker'] if item.startswith(tuple(fake_voices))]))\n",
    "\n",
    "real_speaker_partitions = list(chunks(real_speakers, 20))\n",
    "fake_speaker_partitions = list(chunks(fake_speakers, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40152362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "352"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(real_speakers + fake_speakers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd42262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/data/TIMIT_and_ElevenLabs/16KHz/MPAR0_SI946.WAV'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['path'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad926c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2b8a1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport os\\nimport shutil\\n\\nfile_paths = list(df[\\'orig_path\\'])\\noutput_dir = \\'/home/ubuntu/data/TIMIT_and_ElevenLabs/Original\\'\\n\\ndef copy_files_to_directory(file_paths, destination_directory):\\n    for file_path in file_paths:\\n        # Extract the file name from the path\\n        file_name = os.path.basename(file_path)\\n        \\n        # Create the destination path by joining the destination directory and the file name\\n        destination_path = os.path.join(destination_directory, file_name)\\n        \\n        try:\\n            # Copy the file to the destination directory\\n            shutil.copy2(file_path, destination_path)\\n            print(f\"Successfully copied {file_path} to {destination_path}\")\\n        except FileNotFoundError:\\n            print(f\"File {file_path} not found.\")\\n        except shutil.Error as e:\\n            print(f\"Error occurred while copying {file_path}: {e}\")\\n            \\ncopy_files_to_directory(file_paths, output_dir)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "file_paths = list(df['orig_path'])\n",
    "output_dir = '/home/ubuntu/data/TIMIT_and_ElevenLabs/Original'\n",
    "\n",
    "def copy_files_to_directory(file_paths, destination_directory):\n",
    "    for file_path in file_paths:\n",
    "        # Extract the file name from the path\n",
    "        file_name = os.path.basename(file_path)\n",
    "        \n",
    "        # Create the destination path by joining the destination directory and the file name\n",
    "        destination_path = os.path.join(destination_directory, file_name)\n",
    "        \n",
    "        try:\n",
    "            # Copy the file to the destination directory\n",
    "            shutil.copy2(file_path, destination_path)\n",
    "            print(f\"Successfully copied {file_path} to {destination_path}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File {file_path} not found.\")\n",
    "        except shutil.Error as e:\n",
    "            print(f\"Error occurred while copying {file_path}: {e}\")\n",
    "            \n",
    "copy_files_to_directory(file_paths, output_dir)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3e719b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/data/TIMIT_and_ElevenLabs/TIMIT and ElevenLabs/SX176/fake/Antoni_SX176.wav'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['orig_path'][977]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587f9237",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[df['label'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97346ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport os, shutil\\npaths = df['path'].tolist()\\n\\nfor path in paths:\\n    file_name = os.path.basename(path)\\n    output_path = f'/home/ubuntu/data/TIMIT_and_ElevenLabs/Original/{file_name}'\\n    shutil.copyfile(path, output_path)\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import os, shutil\n",
    "paths = df['path'].tolist()\n",
    "\n",
    "for path in paths:\n",
    "    file_name = os.path.basename(path)\n",
    "    output_path = f'/home/ubuntu/data/TIMIT_and_ElevenLabs/Original/{file_name}'\n",
    "    shutil.copyfile(path, output_path)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad43f646",
   "metadata": {},
   "outputs": [],
   "source": [
    "### End of timit additions ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd569e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(data_df):\n",
    "    speaker_model = nemo_asr.models.EncDecSpeakerLabelModel.from_pretrained(model_name='titanet_large')\n",
    "    embedding_manager = AudioEmbeddingsManager(model=speaker_model, data=data_df)\n",
    "    em_feature_df, em_feature_cols = embedding_manager.generateFeatureDf()\n",
    "    \n",
    "    cadence_manager = CadenceModelManager(data_df)\n",
    "    cad_feature_df, cad_feature_cols, scalar =  cadence_manager.run_cadence_feature_extraction_pipeline(fill_na=-1) # Add param for load features or not\n",
    "    \n",
    "    smile_manager = SmileFeatureManager(data_df)\n",
    "    #change number of features (feature_count=10 default)\n",
    "    os_binary_feature_df, os_binary_feature_cols = smile_manager.generateFeatureDf('random_forest', label_type='binary', feature_count=20)\n",
    "    os_multiclass_feature_df, os_multiclass_feature_cols = smile_manager.generateFeatureDf('random_forest', label_type='multiclass', feature_count=20)\n",
    "    \n",
    "    feature_store = {}\n",
    "    feature_store['titanet'] = (em_feature_df, em_feature_cols)\n",
    "    feature_store['openSmile_binary'] = (os_binary_feature_df, os_binary_feature_cols)\n",
    "    feature_store['openSmile_multiclass'] = (os_multiclass_feature_df, os_multiclass_feature_cols)\n",
    "    feature_store['cadence'] = (cad_feature_df, cad_feature_cols)\n",
    "    \n",
    "    return feature_store\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271e1add",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval(feature_store, fake_cols):\n",
    "    results_cols = ['feature_method', 'model', 'fake_cols', 'label_type', 'acc', 'cls_acc', 'loss', 'eer_score', 'eer_threshold']\n",
    "    results = pd.DataFrame(columns=results_cols)\n",
    "    \n",
    "    models = ['logreg', 'random_forest']\n",
    "    for model in models:\n",
    "        for label_type in ['label', 'multiclass_label']:\n",
    "            for k, v in feature_store.items():\n",
    "                model_manager = ModelManager(model, v[0], v[1], merge_train_dev=True)\n",
    "                model_manager.trainPredict(label_col=label_type)\n",
    "                results = results.append(pd.DataFrame({'feature_method':[k], 'label_type':[label_type], 'fake_cols':[fake_cols], 'acc':[model_manager.accuracy], 'cls_acc':[model_manager.class_accuracy],  'loss':[model_manager.log_loss_value], 'model':[model], \n",
    "                                                       'eer_score':[model_manager.eer_score], 'eer_threshold':[model_manager.eer_threshold]}))\n",
    "    \n",
    "    return results\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f8613a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(fake_cols, metadata_path, name, loader, data_df=None, csv=True, speakers_to_remove=real_speaker_partitions[2]+fake_speaker_partitions[3]):\n",
    "    if data_df is None:\n",
    "        data_df = loader.generate_split_speaker_test(speakers_to_remove)\n",
    "        data_df.to_csv(f'/home/ubuntu/data/results/TIMIT/DATA_DF_{name}.csv')\n",
    "    feature_store = generate_features(data_df)\n",
    "    results = train_eval(feature_store, fake_cols)\n",
    "    \n",
    "    if csv:\n",
    "        results.to_csv(f'/home/ubuntu/data/results/TIMIT/{name}.csv', index=False)\n",
    "    \n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e1a792",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "file_path = '/home/ubuntu/data/wavefake_data/LJ_metadata_16000KHz.csv'\n",
    "# Single results run\n",
    "#run(['ThisDoesNotMatter'], file_path, '16KHz_ElevenLabs_TIMIT_single_fold_TEST', loader=timit_data_loader)\n",
    "\n",
    "# Multiple results run\n",
    "results = []\n",
    "counter = 0 \n",
    "\n",
    "for fake_speaker_chunk in fake_speaker_partitions:\n",
    "    for real_speaker_chunk in real_speaker_partitions:\n",
    "        print(f'progress: {counter}/{len(fake_speaker_partitions)*len(real_speaker_partitions)}')\n",
    "        voices_to_remove = fake_speaker_chunk+real_speaker_chunk\n",
    "        results.append(run(['ThisDoesNotMatter'], file_path, '16KHz_ElevenLabs_TIMIT_k_fold_final', loader=timit_data_loader, csv=False))\n",
    "        counter += 1\n",
    "        clear_output()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6253ed5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results[0])\n",
    "\n",
    "for i in range(len(results[1:])):\n",
    "    results_df = pd.concat([results_df, results[i]])\n",
    "\n",
    "results_df = results_df.reset_index()\n",
    "\n",
    "results_df['class_acc_0'] = results_df['cls_acc'].apply(pd.Series)[0]\n",
    "results_df['class_acc_1'] = results_df['cls_acc'].apply(pd.Series)[1]\n",
    "\n",
    "results_df.groupby(['feature_method', 'model', 'label_type'])[['acc', 'class_acc_0', 'class_acc_1', 'eer_score']].mean().reset_index().to_csv('/home/ubuntu/data/results/TIMIT/16KHz_ElevenLabs_TIMIT_k_fold_regenerated.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acb42bf",
   "metadata": {},
   "source": [
    "# RUN DOWN TO HERE! ^ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c58417",
   "metadata": {},
   "source": [
    "For the columns, we want:\n",
    "- Which dataset are we using\n",
    "- What are the fake cols\n",
    "- Binary/Multi-Class\n",
    "- Feature Generation Method\n",
    "- Accuracy\n",
    "- Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92f7a0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nemo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
